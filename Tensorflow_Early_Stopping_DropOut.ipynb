{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a783939d",
   "metadata": {},
   "source": [
    "# MLP (MNIST, Tensorflow) with Early Stopping\n",
    "\n",
    "In this tutorial, we will apply early stopping on MNIST MLP tensorflow code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a87e34b",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=maSkSt7__Bg\n",
    "\n",
    "https://github.com/minsuk-heo/deeplearning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f79ff",
   "metadata": {},
   "source": [
    "https://ihis.udemy.com/course/pytorch-for-deep-learning-and-computer-vision/learn/lecture/13346688#content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb4c2b4",
   "metadata": {},
   "source": [
    "https://ihis.udemy.com/course/pytorch-for-deep-learning-and-computer-vision/learn/lecture/13346692#overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4734fa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acab26d",
   "metadata": {},
   "source": [
    "# MLP Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de90a239",
   "metadata": {},
   "source": [
    "here is the overview of MLP architecture we will implement with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ce844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/simple_mlp_mnist.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f993ac",
   "metadata": {},
   "source": [
    "# Collect MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff7890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca3444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae538f",
   "metadata": {},
   "source": [
    "train data has **60000** samples\\\n",
    "test data has **10000** samples\\\n",
    "every data is **28 * 28** pixels\\\n",
    "below image shows 28*28 pixel image sample for hand written number '0' from MNIST data.\\\n",
    "MNIST is gray scale image [0 to 255] for hand written number.\n",
    "\n",
    "![0 from MNIST](https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/mnist_sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3825c7",
   "metadata": {},
   "source": [
    "# Split train data into train and validation data\n",
    "\n",
    "Validation during training gives advantages below,\n",
    "1) check if train goes well based on validation score \\\n",
    "2) apply **early stopping** when validation score doesn't improve while train score goes up (overcome **overfitting**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570b25fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val  = x_train[50000:60000]\n",
    "x_train = x_train[0:50000]\n",
    "y_val  = y_train[50000:60000]\n",
    "y_train = y_train[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74498f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train data has \" + str(x_train.shape[0]) + \" samples\")\n",
    "print(\"every train data is \" + str(x_train.shape[1]) \n",
    "      + \" * \" + str(x_train.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a2f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"validation data has \" + str(x_val.shape[0]) + \" samples\")\n",
    "print(\"every train data is \" + str(x_val.shape[1]) \n",
    "      + \" * \" + str(x_train.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a6a6c4",
   "metadata": {},
   "source": [
    "28 * 28 pixels has gray scale value from 0 to 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample to show gray scale values\n",
    "print(x_train[0][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample to show labels for first train data to 10th train data\n",
    "print(y_train[0:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1dabb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test data has \" + str(x_test.shape[0]) + \" samples\")\n",
    "print(\"every test data is \" + str(x_test.shape[1]) \n",
    "      + \" * \" + str(x_test.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74f9f3",
   "metadata": {},
   "source": [
    "# Reshape\n",
    "\n",
    "In order to fully connect all pixels to hidden layer, \\\n",
    "we will reshape (28, 28) into (28x28,1) shape. \\\n",
    "It means we flatten row x column shape to an array having 28x28 (756) items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18117ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/reshape_mnist.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec78bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(50000, 784)\n",
    "x_val = x_val.reshape(10000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12ce292",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08431d48",
   "metadata": {},
   "source": [
    "# Normalize data\n",
    "\n",
    "normalization usually helps faster learning speed, better performance \\\n",
    "by reducing variance and giving same range to all input features. \\\n",
    "since MNIST data set all input has 0 to 255, normalization only helps reducing variances. \\\n",
    "it turned out normalization is better than standardization for MNIST data with my MLP architeture, \\\n",
    "I believe this is because relu handles 0 differently on both feed forward and back propagation. \\\n",
    "handling 0 differently is important for MNIST, since 1-255 means there is some hand written, \\\n",
    "while 0 means no hand written on that pixel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c57f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "gray_scale = 255\n",
    "x_train /= gray_scale\n",
    "x_val /= gray_scale\n",
    "x_test /= gray_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5d8ddf",
   "metadata": {},
   "source": [
    "# label to one hot encoding value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035d6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadad077",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed21450",
   "metadata": {},
   "source": [
    "# Tensorflow MLP Graph\n",
    "\n",
    "Let's implement the MLP graph with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5af9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/simple_mlp_mnist.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa7ebd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f506d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x):\n",
    "    # hidden layer1\n",
    "    w1 = tf.Variable(tf.random_uniform([784,256]))\n",
    "    b1 = tf.Variable(tf.zeros([256]))\n",
    "    h1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "    # hidden layer2\n",
    "    w2 = tf.Variable(tf.random_uniform([256,128]))\n",
    "    b2 = tf.Variable(tf.zeros([128]))\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2) + b2)\n",
    "    # output layer\n",
    "    w3 = tf.Variable(tf.random_uniform([128,10]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "    logits= tf.matmul(h2, w3) + b3\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650f7119",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d680cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=logits, labels=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e8584",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac03fe9",
   "metadata": {},
   "source": [
    "# Early Stopping\n",
    "\n",
    "When validation accuracy doesn't improve while train accuracy keep improves,\\\n",
    "we can early stop the train in order to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a444d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/early_stop.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244bc6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# train hyperparameters\n",
    "epoch_cnt = 300\n",
    "batch_size = 1000\n",
    "iteration = len(x_train) // batch_size\n",
    "\n",
    "earlystop_threshold = 5\n",
    "earlystop_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    prev_train_acc = 0.0\n",
    "    max_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epoch_cnt):\n",
    "        avg_loss = 0.\n",
    "        start = 0; end = batch_size\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            _, loss = sess.run([train_op, loss_op], \n",
    "                               feed_dict={x: x_train[start: end], y: y_train[start: end]})\n",
    "            start += batch_size; end += batch_size\n",
    "            # Compute train average loss\n",
    "            avg_loss += loss / iteration\n",
    "            \n",
    "        # Validate model\n",
    "        preds = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        # train accuracy\n",
    "        cur_train_acc = accuracy.eval({x: x_train, y: y_train})\n",
    "        # validation accuarcy\n",
    "        cur_val_acc = accuracy.eval({x: x_val, y: y_val})\n",
    "        # validation loss\n",
    "        cur_val_loss = loss_op.eval({x: x_val, y: y_val})\n",
    "        \n",
    "        print(\"epoch: \"+str(epoch)+\n",
    "              \", train acc: \" + str(cur_train_acc) +\n",
    "              \", val acc: \" + str(cur_val_acc) )\n",
    "              #', train loss: '+str(avg_loss)+\n",
    "              #', val loss: '+str(cur_val_loss))\n",
    "        \n",
    "        if cur_val_acc < max_val_acc:\n",
    "            if cur_train_acc > prev_train_acc or cur_train_acc > 0.99:\n",
    "                if earlystop_cnt == earlystop_threshold:\n",
    "                    print(\"early stopped on \"+str(epoch))\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"overfitting warning: \"+str(earlystop_cnt))\n",
    "                    earlystop_cnt += 1\n",
    "            else:\n",
    "                earlystop_cnt = 0\n",
    "        else:\n",
    "            earlystop_cnt = 0\n",
    "            max_val_acc = cur_val_acc\n",
    "            # Save the variables to file.\n",
    "            save_path = saver.save(sess, \"model/model.ckpt\")\n",
    "        prev_train_acc = cur_train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb3dc44",
   "metadata": {},
   "source": [
    "# Testing with the best epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24db682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start testing\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"model/model.ckpt\")\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"[Test Accuracy] :\", accuracy.eval({x: x_test, y: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85143b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9623a6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
